{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fab6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement google.colab (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for google.colab\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12e1c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m      3\u001b[39m login(userdata.get(\u001b[33m'\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(userdata.get('HF_TOKEN'))\n",
    "token = userdata.get('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-270m-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    token = token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ec9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\", token=token)\n",
    "ds = ds.shuffle()\n",
    "train_ds = ds[\"train\"].select(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Given the <USER_QUERY> and the <SCHEMA>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n",
    "\n",
    "<SCHEMA>\n",
    "{context}\n",
    "</SCHEMA>\n",
    "\n",
    "<USER_QUERY>\n",
    "{question}\n",
    "</USER_QUERY>\n",
    "\n",
    "<RESPONSE>\n",
    "{response}\n",
    "</RESPONSE>\n",
    "\"\"\"\n",
    "\n",
    "def format_prompts_for_training(examples):\n",
    "    return {\"prompt\": user_prompt.format(question=examples[\"sql_prompt\"], context=examples[\"sql_context\"], response=examples[\"sql\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"prompt\",\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-5,\n",
    "        logging_steps = 100,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir=\"./model/gemma-270m-Text2SQL-Fine-tuned\",\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"Gemma-Text2SQL\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d491490",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=Gemma-Text2SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c99862",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adf08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\"leotod/gemma-270m-Text2SQL-Fine-tuned\", tokenizer, save_method = \"merged_16bit\", token = token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
